{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_regions_of_Canada'\n",
    "df_can = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "canada = list(df_can.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States'\n",
    "df_usa = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "usa = list(df_usa.iloc[:,0].apply(lambda x : x.replace(\"[E]\",\"\").replace(\"[F]\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Countries_of_the_United_Kingdom'\n",
    "df_uk = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "uk = list(df_uk.iloc[:-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/States_and_territories_of_Australia'\n",
    "df_aus = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "australia = list(df_aus[df_aus.iloc[:,1] == 'Federated state'].iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_can = f'https://ca.indeed.com/jobs?'\n",
    "url_usa = f'https://www.indeed.com/jobs?'\n",
    "url_uk = f'https://www.indeed.co.uk/jobs?'\n",
    "url_aus = f'https://au.indeed.com/jobs?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [canada,usa,uk,australia]\n",
    "countries = [\"Canada\",\"United States\",\"United Kingdom\",\"Australia\"]\n",
    "links = [url_can,url_usa,url_uk,url_aus]\n",
    "\n",
    "country_dict = {}\n",
    "\n",
    "for country,state,link in zip(countries,states,links):\n",
    "    country_dict[country] = [state]\n",
    "    country_dict[country].append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ca.indeed.com/jobs?'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_dict['Canada'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_to_use = ['Queensland','New South Wales','Victoria','South Australia','Western Australia','Tasmania','Northern Territory']\n",
    "\n",
    "\n",
    "list_of_searchwords = ['data+scientist','data+analyst','research+scientist','business+intelligence','machine+learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Determining number of jobs per state,region, area etc\n",
    "\n",
    "global_jobs = {'country':[],'state':[],'role':[],'jobs':[]}\n",
    "\n",
    "for country in tqdm_notebook(countries):\n",
    "\n",
    "    for state in tqdm_notebook(country_dict[country][0]):\n",
    "\n",
    "        for role in list_of_searchwords:\n",
    "\n",
    "            count_url = country_dict[country][1]+'as_ttl={role}&jt=all&radius=5000&l={state}&fromage=any&limit=50&filter=0'\n",
    "\n",
    "            time.sleep(np.random.randint(0,2))\n",
    "\n",
    "            counter = requests.get(count_url)\n",
    "            soup_count = BeautifulSoup(counter.text,'html.parser')\n",
    "\n",
    "            if len(soup_count.find_all('div',class_='bad_query'))==1:\n",
    "\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                try:\n",
    "                    record_string = soup_count.find_all('div',attrs={'id':\"searchCountPages\"})[0].text\n",
    "                    record_string = record_string.replace(\",\",\"\")\n",
    "                    max_results_per_city = int(re.search(r\"(\\w+)\\sjobs\",record_string).group(1))\n",
    "\n",
    "                except:\n",
    "                    max_results_per_city = np.nan\n",
    "\n",
    "                #try:\n",
    "                    #true_pages = int(soup_count.find_all(\"span\",class_=\"pn\")[-2].text)\n",
    "                    #print(true_pages,max_results_per_city)\n",
    "\n",
    "                #except:\n",
    "                    #true_pages = 0\n",
    "                    #print(true_pages)\n",
    "\n",
    "                global_jobs['country'].append(country)\n",
    "                global_jobs['state'].append(state)\n",
    "                global_jobs['role'].append(role)\n",
    "                global_jobs['jobs'].append(max_results_per_city)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_nums = pd.DataFrame(jobs_in_state).sort_values(by='jobs',ascending=False)\n",
    "\n",
    "state_searchlist= job_nums.groupby([\"state\"]).mean().sort_values(by=\"jobs\",ascending=True).index.to_list()\n",
    "role_searchlist = job_nums.groupby([\"role\"]).mean().sort_values(by=\"jobs\",ascending=True).index.to_list()\n",
    "job_nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'job_title':[],'job_category':[],\n",
    "           'company':[],'location':[],\n",
    "           'region':[],'salary':[],\n",
    "           'link':[],'summary':[]}\n",
    "\n",
    "#url_template = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "max_results_per_city = 294 # Set this to a high-value (5000) to generate more results. \n",
    "# Crawling more results, will also take much longer. First test your code on a small number of results and then expand.\n",
    "\n",
    "all_files = []\n",
    "\n",
    "for state in tqdm_notebook(state_searchlist):\n",
    "\n",
    "    for role in tqdm_notebook(role_searchlist):\n",
    "        \n",
    "        count_url = f'https://au.indeed.com/jobs?as_ttl={role}&jt=all&radius=5000&l={state}&fromage=any&limit=50&start=0&filter=0'\n",
    "\n",
    "        time.sleep(np.random.randint(0,1))\n",
    "\n",
    "        counter = requests.get(count_url)\n",
    "        soup_count = BeautifulSoup(counter.text,'html.parser')\n",
    "        \n",
    "        if len(soup_count.find_all('div',class_='bad_query'))==1:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        else:     \n",
    "\n",
    "            try:\n",
    "                record_string = soup_count.find_all('div',attrs={'id':\"searchCountPages\"})[0].text\n",
    "                record_string = record_string.replace(\",\",\"\")\n",
    "                max_results_per_city = int(re.search(r\"(\\w+)\\sjobs\",record_string).group(1))\n",
    "            except:\n",
    "                max_results_per_city = 1\n",
    "\n",
    "            for start in tqdm_notebook(range(0, max_results_per_city,50)):\n",
    "                time.sleep(np.random.randint(0,1))\n",
    "\n",
    "                URL = f'https://au.indeed.com/jobs?as_ttl={role}&jt=all&radius=5000&l={state}&fromage=any&limit=50&start={start}&filter=0'\n",
    "\n",
    "                results = requests.get(URL)\n",
    "                soup = BeautifulSoup(results.text,'html.parser')\n",
    "\n",
    "                #print(city,int(max_results_per_city))\n",
    "\n",
    "                #print(state,start,soup.find_all('div',attrs={'id':\"searchCountPages\"})[0].text)\n",
    "\n",
    "                if len(soup.find_all('span',attrs={'class':'salary no-wrap'})) == 0:\n",
    "\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    for jobtile in soup.find_all('div',attrs={'class':re.compile(r'^jobsearch-Serp.*unified.*')}):\n",
    "\n",
    "                        if len(jobtile.find_all('span',attrs={'class':'salary no-wrap'})) == 0:\n",
    "                            pass\n",
    "                        else:\n",
    "                            all_files.append(jobtile)\n",
    "                            df_dict['job_category'].append(\" \".join(role.split(\"+\")).title())\n",
    "                            df_dict['region'].append(\" \".join(state.split(\"+\")).title())\n",
    "\n",
    "                            detail_link = \"https://au.indeed.com/\"+extract_href_from_result(jobtile)\n",
    "                            df_dict['link'].append(detail_link)\n",
    "                            time.sleep(np.random.randint(0,1))\n",
    "                            detailgrab = requests.get(detail_link)\n",
    "                            soup_detail = BeautifulSoup(detailgrab.text,'html.parser')\n",
    "\n",
    "                            try:\n",
    "                                df_dict['summary'].append(extract_summary_from_result(soup_detail).text)\n",
    "                            except:\n",
    "                                df_dict['summary'].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in all_files:\n",
    "    try:\n",
    "        df_dict['location'].append(extract_location_from_result(x))\n",
    "    except:\n",
    "        df_dict['location'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        df_dict['company'].append(extract_company_from_result(x))\n",
    "    except:\n",
    "        df_dict['company'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        df_dict['salary'].append(extract_salary_from_result(x))\n",
    "        \n",
    "    except:\n",
    "        df_dict['salary'].append(np.nan)\n",
    "\n",
    "    try:\n",
    "        df_dict['job_title'].append(extract_jobtitle_from_result(x))\n",
    "        \n",
    "    except:\n",
    "        df_dict['job_title'].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_dict)\n",
    "\n",
    "np.sum(df[df.salary.notna()].duplicated() ==False)*100/df.shape[0]\n",
    "\n",
    "#df = df[df.salary.notna()]\n",
    "\n",
    "#df = df[df.duplicated()==False]\n",
    "\n",
    "df.to_csv(\"aus_jobs.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
