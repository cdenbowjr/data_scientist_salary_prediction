{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_regions_of_Canada'\n",
    "df_can = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "canada = list(df_can.iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States'\n",
    "df_usa = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "usa = list(df_usa.iloc[:,0].apply(lambda x : x.replace(\"[E]\",\"\").replace(\"[F]\",\"\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Countries_of_the_United_Kingdom'\n",
    "df_uk = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "uk = list(df_uk.iloc[:-1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/States_and_territories_of_Australia'\n",
    "df_aus = pd.read_html(url, attrs={\"class\": \"wikitable\"})[0] # 0 is for the 1st table in this particular page\n",
    "australia = list(df_aus[df_aus.iloc[:,1] == 'Federated state'].iloc[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_can = f'https://ca.indeed.com/jobs?'\n",
    "url_usa = f'https://www.indeed.com/jobs?'\n",
    "url_uk = f'https://www.indeed.co.uk/jobs?'\n",
    "url_aus = f'https://au.indeed.com/jobs?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['British Columbia',\n",
       " 'Alberta',\n",
       " 'Saskatchewan',\n",
       " 'Manitoba',\n",
       " 'Ontario',\n",
       " 'Quebec',\n",
       " 'New Brunswick',\n",
       " 'Prince Edward Island',\n",
       " 'Nova Scotia',\n",
       " 'Newfoundland and Labrador',\n",
       " 'Yukon',\n",
       " 'Northwest Territories',\n",
       " 'Nunavut']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "canada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = [canada,usa,uk,australia]\n",
    "countries = [\"Canada\",\"United States\",\"United Kingdom\",\"Australia\"]\n",
    "links = [url_can,url_usa,url_uk,url_aus]\n",
    "\n",
    "country_dict = {}\n",
    "\n",
    "for country,state,link in zip(countries,states,links):\n",
    "    country_dict[country] = [state]\n",
    "    country_dict[country].append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://ca.indeed.com/jobs?'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "country_dict['Canada'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_searchwords = ['data+scientist','data+analyst','research+scientist','business+intelligence','machine+learning']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "462f8a53cc2144d0ac3b0b58dee07a7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Canada\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2606dbdba374c80964c8c873600fbfe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=13), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "British Columbia\n",
      "Alberta\n",
      "Saskatchewan\n",
      "Manitoba\n",
      "Ontario\n",
      "Quebec\n",
      "New Brunswick\n",
      "Prince Edward Island\n",
      "Nova Scotia\n",
      "Newfoundland and Labrador\n",
      "Yukon\n",
      "Northwest Territories\n",
      "Nunavut\n",
      "United States\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fab0a5aa484c4508999ed948b88c2ef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=50), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alabama\n",
      "Alaska\n",
      "Arizona\n",
      "Arkansas\n",
      "California\n",
      "Colorado\n",
      "Connecticut\n",
      "Delaware\n",
      "Florida\n",
      "Georgia\n",
      "Hawaii\n",
      "Idaho\n",
      "Illinois\n",
      "Indiana\n",
      "Iowa\n",
      "Kansas\n",
      "Kentucky\n",
      "Louisiana\n",
      "Maine\n",
      "Maryland\n",
      "Massachusetts\n",
      "Michigan\n",
      "Minnesota\n",
      "Mississippi\n",
      "Missouri\n",
      "Montana\n",
      "Nebraska\n",
      "Nevada\n",
      "New Hampshire\n",
      "New Jersey\n",
      "New Mexico\n",
      "New York\n",
      "North Carolina\n",
      "North Dakota\n",
      "Ohio\n",
      "Oklahoma\n",
      "Oregon\n",
      "Pennsylvania\n",
      "Rhode Island\n",
      "South Carolina\n",
      "South Dakota\n",
      "Tennessee\n",
      "Texas\n",
      "Utah\n",
      "Vermont\n",
      "Virginia\n",
      "Washington\n",
      "West Virginia\n",
      "Wisconsin\n",
      "Wyoming\n",
      "United Kingdom\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39d0a42a6ee1483e9eb48676dada4097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "England\n",
      "Northern Ireland\n",
      "Scotland\n",
      "Wales\n",
      "Australia\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2086cd9e96c4f9fbf407f59bb2e1d33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=6), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "South Australia\n",
      "Tasmania\n",
      "New South Wales\n",
      "Victoria\n",
      "Western Australia\n",
      "Queensland\n"
     ]
    }
   ],
   "source": [
    "#Determining number of jobs per state,region, area etc\n",
    "\n",
    "global_jobs = {'country':[],'state':[],'role':[],'jobs':[]}\n",
    "\n",
    "for country in tqdm_notebook(countries):\n",
    "    print(country)    \n",
    "\n",
    "    for state in tqdm_notebook(country_dict[country][0]):\n",
    "        print(state)\n",
    "\n",
    "        for role in list_of_searchwords:\n",
    "\n",
    "            count_url = country_dict[country][1]+f'as_ttl={role}&jt=all&radius=5000&l={state}&fromage=any&limit=50&filter=0'\n",
    "            time.sleep(np.random.randint(0,2))\n",
    "\n",
    "            counter = requests.get(count_url)\n",
    "            soup_count = BeautifulSoup(counter.text,'html.parser')\n",
    "            \n",
    "\n",
    "            if len(soup_count.find_all('div',class_='bad_query'))==1:\n",
    "                #print(\"bad query\")\n",
    "\n",
    "                pass\n",
    "\n",
    "            else:\n",
    "\n",
    "                try:\n",
    "                    record_string = soup_count.find_all('div',attrs={'id':\"searchCountPages\"})[0].text\n",
    "                    record_string = record_string.replace(\",\",\"\")\n",
    "                    max_results_per_city = int(re.search(r\"(\\w+)\\sjobs\",record_string).group(1))\n",
    "                    #print(max_results_per_city)\n",
    "\n",
    "                except:\n",
    "                    max_results_per_city = np.nan\n",
    "                    #print(\"NAN\")\n",
    "\n",
    "                #try:\n",
    "                    #true_pages = int(soup_count.find_all(\"span\",class_=\"pn\")[-2].text)\n",
    "                    #print(true_pages,max_results_per_city)\n",
    "\n",
    "                #except:\n",
    "                    #true_pages = 0\n",
    "                    #print(true_pages)\n",
    "\n",
    "                global_jobs['country'].append(country)\n",
    "                global_jobs['state'].append(state)\n",
    "                global_jobs['role'].append(role)\n",
    "                global_jobs['jobs'].append(max_results_per_city)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>country</th>\n",
       "      <th>state</th>\n",
       "      <th>role</th>\n",
       "      <th>jobs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>England</td>\n",
       "      <td>data+analyst</td>\n",
       "      <td>1612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>data+scientist</td>\n",
       "      <td>938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>data+analyst</td>\n",
       "      <td>935</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>United Kingdom</td>\n",
       "      <td>England</td>\n",
       "      <td>data+scientist</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>United States</td>\n",
       "      <td>California</td>\n",
       "      <td>machine+learning</td>\n",
       "      <td>708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            country       state              role  jobs\n",
       "269  United Kingdom     England      data+analyst  1612\n",
       "52    United States  California    data+scientist   938\n",
       "53    United States  California      data+analyst   935\n",
       "268  United Kingdom     England    data+scientist   914\n",
       "56    United States  California  machine+learning   708"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "job_nums = pd.DataFrame(global_jobs).sort_values(by='jobs',ascending=False)\n",
    "\n",
    "country_searchlist = job_nums.groupby([\"country\"]).mean().sort_values(by=\"jobs\",ascending=True).index.to_list()\n",
    "#state_searchlist= job_nums.groupby([\"state\"]).mean().sort_values(by=\"jobs\",ascending=True).index.to_list()\n",
    "#role_searchlist = job_nums.groupby([\"role\"]).mean().sort_values(by=\"jobs\",ascending=True).index.to_list()\n",
    "job_nums.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dict = {'job_title':[],'job_category':[],\n",
    "           'company':[],'location':[],\n",
    "           'region':[],'salary':[],\n",
    "           'link':[],'summary':[]}\n",
    "\n",
    "#url_template = \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l={}&start={}\"\n",
    "max_results_per_city = 294 # Set this to a high-value (5000) to generate more results. \n",
    "# Crawling more results, will also take much longer. First test your code on a small number of results and then expand.\n",
    "\n",
    "all_files = []\n",
    "\n",
    "for state in tqdm_notebook(state_searchlist):\n",
    "\n",
    "    for role in tqdm_notebook(role_searchlist):\n",
    "        \n",
    "        count_url = f'https://au.indeed.com/jobs?as_ttl={role}&jt=all&radius=5000&l={state}&fromage=any&limit=50&start=0&filter=0'\n",
    "\n",
    "        time.sleep(np.random.randint(0,1))\n",
    "\n",
    "        counter = requests.get(count_url)\n",
    "        soup_count = BeautifulSoup(counter.text,'html.parser')\n",
    "        \n",
    "        if len(soup_count.find_all('div',class_='bad_query'))==1:\n",
    "            \n",
    "            pass\n",
    "        \n",
    "        else:     \n",
    "\n",
    "            try:\n",
    "                record_string = soup_count.find_all('div',attrs={'id':\"searchCountPages\"})[0].text\n",
    "                record_string = record_string.replace(\",\",\"\")\n",
    "                max_results_per_city = int(re.search(r\"(\\w+)\\sjobs\",record_string).group(1))\n",
    "            except:\n",
    "                max_results_per_city = 1\n",
    "\n",
    "            for start in tqdm_notebook(range(0, max_results_per_city,50)):\n",
    "                time.sleep(np.random.randint(0,1))\n",
    "\n",
    "                URL = f'https://au.indeed.com/jobs?as_ttl={role}&jt=all&radius=5000&l={state}&fromage=any&limit=50&start={start}&filter=0'\n",
    "\n",
    "                results = requests.get(URL)\n",
    "                soup = BeautifulSoup(results.text,'html.parser')\n",
    "\n",
    "                #print(city,int(max_results_per_city))\n",
    "\n",
    "                #print(state,start,soup.find_all('div',attrs={'id':\"searchCountPages\"})[0].text)\n",
    "\n",
    "                if len(soup.find_all('span',attrs={'class':'salary no-wrap'})) == 0:\n",
    "\n",
    "                    pass\n",
    "\n",
    "                else:\n",
    "                    for jobtile in soup.find_all('div',attrs={'class':re.compile(r'^jobsearch-Serp.*unified.*')}):\n",
    "\n",
    "                        if len(jobtile.find_all('span',attrs={'class':'salary no-wrap'})) == 0:\n",
    "                            pass\n",
    "                        else:\n",
    "                            all_files.append(jobtile)\n",
    "                            df_dict['job_category'].append(\" \".join(role.split(\"+\")).title())\n",
    "                            df_dict['region'].append(\" \".join(state.split(\"+\")).title())\n",
    "\n",
    "                            detail_link = \"https://au.indeed.com/\"+extract_href_from_result(jobtile)\n",
    "                            df_dict['link'].append(detail_link)\n",
    "                            time.sleep(np.random.randint(0,1))\n",
    "                            detailgrab = requests.get(detail_link)\n",
    "                            soup_detail = BeautifulSoup(detailgrab.text,'html.parser')\n",
    "\n",
    "                            try:\n",
    "                                df_dict['summary'].append(extract_summary_from_result(soup_detail).text)\n",
    "                            except:\n",
    "                                df_dict['summary'].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in all_files:\n",
    "    try:\n",
    "        df_dict['location'].append(extract_location_from_result(x))\n",
    "    except:\n",
    "        df_dict['location'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        df_dict['company'].append(extract_company_from_result(x))\n",
    "    except:\n",
    "        df_dict['company'].append(np.nan)\n",
    "        \n",
    "    try:\n",
    "        df_dict['salary'].append(extract_salary_from_result(x))\n",
    "        \n",
    "    except:\n",
    "        df_dict['salary'].append(np.nan)\n",
    "\n",
    "    try:\n",
    "        df_dict['job_title'].append(extract_jobtitle_from_result(x))\n",
    "        \n",
    "    except:\n",
    "        df_dict['job_title'].append(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(df_dict)\n",
    "\n",
    "np.sum(df[df.salary.notna()].duplicated() ==False)*100/df.shape[0]\n",
    "\n",
    "#df = df[df.salary.notna()]\n",
    "\n",
    "#df = df[df.duplicated()==False]\n",
    "\n",
    "df.to_csv(\"aus_jobs.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.duplicated()==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
